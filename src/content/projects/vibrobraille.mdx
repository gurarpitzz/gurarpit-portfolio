---
title: "VibroBraille Hybrid"
subtitle: "Braille-First AI Reading Interface â€¢ Feb 2024"
description: "A highly specialized assistive system designed for sequential vibration patterns on smartphones, translating semantic meaning into tactile signals for the visually impaired."
date: "2024-02-15"
tags: ["Accessibility", "Signal Encoding", "Python"]
github: "https://github.com/gurarpitzz/Vibrobraille-hybrid"
features:
  - "Real-time haptic vibration encoding"
  - "Semantic text simplification AI"
  - "WebSocket-based low-latency relay"
  - "Dynamic playback & density control"
techStack: ["Python", "WebSockets", "OpenAI", "React Native", "Flask"]
---

VibroBraille bridges the gap between digital content and tactile accessibility. Unlike traditional screen readers that rely on audio, VibroBraille utilizes the smartphone's haptic engine to deliver "tactile sentences," allowing users to read in noisy environments or with complete privacy.

## The Challenge
Reading physical or digital documents as a visually impaired individual often requires expensive hardware or intrusive audio feedback. VibroBraille aims to democratize access by using standard smartphone hardware.

## Engineering Highlight: Haptic Relay
The system features a **PC-to-Mobile relay** that processes text logic on a high-compute backend and streams haptic actuation signals to the mobile device. This ensures that even complex NLP processing (like text simplification) doesn't drain the mobile battery or introduce latency.

## Impact
Designed with a "Braille-First" philosophy, the system focuses on the rhythmic patterns of language, translating semantic density into vibration pulses that are intuitive for Braille readers.
